{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3959122b",
   "metadata": {},
   "source": [
    "# Vision Zero, Chicago, modeling car crashes with injuries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52999b83",
   "metadata": {},
   "source": [
    "# Overview\n",
    "I was hired by a government agency CMAP (Chicago Metropolitan Agency for Planning) to create a model which predicts injuries during car crashes based on information collected by Chicago Police Department (CPD). I  concluded that there are many features that determine the oucome of car crash(overall 200 features, only 31 were used in our model), some of them: type of crash , day, season, time, type of crash, are there injuried people, phyesical imparement factors(drugs, alchogol, distraction etc), speed limit, weather, lightning and road conditions etc .\n",
    "\n",
    "To acheive my goal, I trained more than 100 models and tuned hyperparameters, model types that were used: \n",
    "- LogisticRegression\n",
    "- KNN\n",
    "- Naive Bayes(different type) \n",
    "- DecisionTree\n",
    "- Random Forest\n",
    "- ADA Boost\n",
    "- Gradient boost\n",
    "- XGB Classifier\n",
    "\n",
    "\n",
    "# Business Understanding\n",
    "Our stakeholder wants to understand what factors of crash influence on injury outcome as the last possible outcome. They perfectly understand that car crashes will be happening but they want to reduce the number of injuries because individual health prevails under public mobility. \n",
    "\n",
    "# Data\n",
    "\n",
    "1) Database Traffic Crashes - Crashes. Years: 2017 - now\n",
    "Provided by City of Chicago\n",
    "\n",
    "https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if \n",
    "\n",
    "2) Database Traffic Crashes - People. Years: 2017 - now\n",
    "Provided by City of Chicago\n",
    "\n",
    "https://data.cityofchicago.org/Transportation/Traffic-Crashes-People/u6pd-qa9d\n",
    "\n",
    "3) Database Traffic Crashes - Vehicles. Years: 2017 - now\n",
    "Provided by City of Chicago  \n",
    "\n",
    "https://data.cityofchicago.org/Transportation/Traffic-Crashes-Vehicles/68nd-jvt3\n",
    "\n",
    "##### In my research I used data for 2021 year.\n",
    "\n",
    "# Metrics\n",
    "#### Our project will answer following question:\n",
    "What factors influence injuries?\n",
    "\n",
    "#### Hypothesis:\n",
    "H0 - car crashes with injuries are random\n",
    "\n",
    "HA - There is a significant dependancy between injuries and the features of dataset\n",
    "\n",
    "#### TP, TN, FP, FN definition\n",
    "TP - we predict car crash with injury and it actually happened.\n",
    "\n",
    "TN - we predicted that there is no injury and there was no injury,\n",
    "\n",
    "FP - We predicted injury but there was no injury in real life\n",
    "\n",
    "FN - We predicted that there will be no injury but it happened\n",
    "\n",
    "\n",
    "#### Metrics used  \n",
    "To compare models we will focus on 2 major metrics:\n",
    "\n",
    "Accuracy - how good we can predict TP and TN. General metrics that will show model performance.\n",
    "\n",
    "Recall - Health of people is our priority, we will be focused to minimize FN, so we can consider as much real car crashes with injuries in our model as possible, even if our model mark some car crashes with injuries but there will be no such. From the other side we need consider accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcdcd20",
   "metadata": {},
   "source": [
    "\n",
    "# Data Understanding\n",
    "#### Sources of data:\n",
    "1) Database Traffic Crashes - Crashes. Years: 2017 - now\n",
    "Covers: Main characteristics of car crash.\n",
    "Provided by City of Chicago \n",
    "https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if\n",
    "\n",
    "2) Database Traffic Crashes - People. Years: 2017 - now\n",
    "Covers: People/drivers involved in car crash.\n",
    "Provided by City of Chicago \n",
    "https://data.cityofchicago.org/Transportation/Traffic-Crashes-People/u6pd-qa9d\n",
    "\n",
    "3) Database Traffic Crashes - Vehicles. Years: 2017 - now\n",
    "Covers: Vehicles involved in car crash.\n",
    "Provided by City of Chicago \n",
    "https://data.cityofchicago.org/Transportation/Traffic-Crashes-Vehicles/68nd-jvt3\n",
    "\n",
    "#### Main dataset contains the following columns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24102ee3",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "### Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b0985f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats as stats\n",
    "\n",
    "# Visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Modeling\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB, GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate, KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report, roc_curve, plot_roc_curve, roc_auc_score, accuracy_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "#Other\n",
    "import pickle\n",
    "import time\n",
    "import gzip\n",
    "import os, shutil \n",
    "from zipfile import ZipFile\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning, FitFailedWarning\n",
    "simplefilter(action='ignore', category= FutureWarning)\n",
    "simplefilter(action='ignore', category= ConvergenceWarning)\n",
    "simplefilter(action='ignore', category= FitFailedWarning)\n",
    "simplefilter(action='ignore', category= UserWarning)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9508df54",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e9ea06",
   "metadata": {},
   "source": [
    "Below we create three objects representing the existing directories: `data/normal/` as `data_normal_dir` and `data/pneumonia/` as `data_pneumonia_dir`, `data/test/normal/` as `test/normal` and `data/test/pneumonia/` as `test/pneumonia`. We will create a new directory `split/` as `new_dir`, where we will split the dataset in three groups (or three subdirectories): `train`, `test`, and `validation`, each containing `normal` and `pneumonia` subfolders. The final desired structure is represented below: \n",
    "\n",
    "![title](images/folder_structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92b77a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normal_dir = 'data/normal/'\n",
    "data_pneumonia_dir = 'data/pneumonia/'\n",
    "new_dir = 'data/split/'\n",
    "data_test_normal_dir = 'data/test/normal/'\n",
    "data_test_pneumonia_dir = 'data/test/pneumonia/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d45e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9928bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "imgs_normal = [file for file in os.listdir(data_normal_dir) if file.endswith('.jpeg')]\n",
    "imgs_pneumonia = [file for file in os.listdir(data_pneumonia_dir) if file.endswith('.jpeg')]\n",
    "# Test set\n",
    "imgs_normal_test = [file for file in os.listdir(data_test_normal_dir) if file.endswith('.jpeg')]\n",
    "imgs_pneumonia_test = [file for file in os.listdir(data_test_pneumonia_dir) if file.endswith('.jpeg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b95fe98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NORMAL-2552119-0002.jpeg'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_normal_dir)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceeb3b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:\n",
      "There are 620 normal images, image name example, NORMAL-2552119-0002.jpeg\n",
      "There are 667 pneumonia images, image name example, BACTERIA-292199-0002.jpeg\n",
      "Test set:\n",
      "There are 234 normal images, image name example, NORMAL-8698006-0001.jpeg\n",
      "There are 390 pneumonia images, image name example, VIRUS-2040583-0001.jpeg\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set:\")\n",
    "print('There are', len(imgs_normal), 'normal images, image name example,',os.listdir(data_normal_dir)[0])\n",
    "print('There are', len(imgs_pneumonia), 'pneumonia images, image name example,',os.listdir(data_pneumonia_dir)[0])\n",
    "print(\"Test set:\")\n",
    "print('There are', len(imgs_normal_test), 'normal images, image name example,',os.listdir(data_test_normal_dir)[0])\n",
    "print('There are', len(imgs_pneumonia_test), 'pneumonia images, image name example,',os.listdir(data_test_pneumonia_dir)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99bf930",
   "metadata": {},
   "source": [
    "Make new split directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c44d51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c2952",
   "metadata": {},
   "source": [
    "Create new directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eafbd099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path\n",
    "train_folder = os.path.join(new_dir, 'train')\n",
    "train_normal = os.path.join(train_folder, 'normal')\n",
    "train_pneumonia = os.path.join(train_folder, 'pneumonia')\n",
    "\n",
    "test_folder = os.path.join(new_dir, 'test')\n",
    "test_normal = os.path.join(test_folder, 'normal')\n",
    "test_pneumonia = os.path.join(test_folder, 'pneumonia')\n",
    "\n",
    "val_folder = os.path.join(new_dir, 'validation')\n",
    "val_normal = os.path.join(val_folder, 'normal')\n",
    "val_pneumonia = os.path.join(val_folder, 'pneumonia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b44cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/split/train/pneumonia'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that path is ok\n",
    "train_pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e252d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "os.mkdir(test_folder)\n",
    "os.mkdir(test_normal)\n",
    "os.mkdir(test_pneumonia)\n",
    "\n",
    "os.mkdir(train_folder)\n",
    "os.mkdir(train_normal)\n",
    "os.mkdir(train_pneumonia)\n",
    "\n",
    "os.mkdir(val_folder)\n",
    "os.mkdir(val_normal)\n",
    "os.mkdir(val_pneumonia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c4e9d1",
   "metadata": {},
   "source": [
    "Copy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea7c6186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting size of validation set\n",
    "val_set = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b539c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f019349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train normal\n",
    "imgs_train_norm = imgs_normal[:len(os.listdir(data_normal_dir))-val_set]\n",
    "for img in imgs_train_norm:\n",
    "    origin = os.path.join(data_normal_dir, img)\n",
    "    destination = os.path.join(train_normal, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "# train pneumonia\n",
    "imgs_train_pneumonia = imgs_pneumonia[:len(os.listdir(data_pneumonia_dir))-val_set]\n",
    "for img in imgs_train_pneumonia:\n",
    "    origin = os.path.join(data_pneumonia_dir, img)\n",
    "    destination = os.path.join(train_pneumonia, img)\n",
    "    shutil.copyfile(origin, destination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c840873c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imgs_train_pneumonia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34b0a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation santa\n",
    "imgs = imgs_normal[len(os.listdir(data_normal_dir))-val_set:]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_normal_dir, img)\n",
    "    destination = os.path.join(val_normal, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "\n",
    "imgs = imgs_pneumonia[len(os.listdir(data_normal_dir))-val_set:]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_pneumonia_dir, img)\n",
    "    destination = os.path.join(val_pneumonia, img)\n",
    "    shutil.copyfile(origin, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d9f35d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test normal\n",
    "imgs = imgs_normal_test\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_test_normal_dir, img)\n",
    "    destination = os.path.join(test_normal, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "# test pneumonia\n",
    "imgs = imgs_pneumonia_test\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_test_pneumonia_dir, img)\n",
    "    destination = os.path.join(test_pneumonia, img)\n",
    "    shutil.copyfile(origin, destination)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dac667",
   "metadata": {},
   "source": [
    "Check the folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba8301c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train images\n",
    "train_img_number = len(os.listdir(train_normal)) + len(os.listdir(train_pneumonia))\n",
    "# Validation images\n",
    "val_img_number = len(os.listdir(val_normal)) + len(os.listdir(val_pneumonia))\n",
    "# Test images\n",
    "test_img_number = len(os.listdir(test_normal)) + len(os.listdir(test_pneumonia))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0d687f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 987 images\n",
      "There are 470 normal images in the training set\n",
      "There are 517 pneumonia images in the training set\n",
      "Validation set: 347 images\n",
      "There are 150 normal images in the validation set\n",
      "There are 197 pneumonia images in the validation set\n",
      "Test set: 624 images\n",
      "There are 234 normal images in the test set\n",
      "There are 390 pneumonia images in the test set\n"
     ]
    }
   ],
   "source": [
    "print('Training set:',train_img_number, \"images\" )\n",
    "print('There are', len(os.listdir(train_normal)), 'normal images in the training set')\n",
    "print('There are', len(os.listdir(train_pneumonia)), 'pneumonia images in the training set')\n",
    "print('Validation set:', val_img_number, \"images\" )\n",
    "print('There are', len(os.listdir(val_normal)), 'normal images in the validation set')\n",
    "print('There are', len(os.listdir(val_pneumonia)), 'pneumonia images in the validation set')\n",
    "print('Test set:', test_img_number, 'images')\n",
    "print('There are', len(os.listdir(test_normal)), 'normal images in the test set')\n",
    "print('There are', len(os.listdir(test_pneumonia)), 'pneumonia images in the test set')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb59494",
   "metadata": {},
   "source": [
    "### Use a densely connected network as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df8f57b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 987 images belonging to 2 classes.\n",
      "Found 347 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# get all the data in the directory split/train (987 images), and reshape them\n",
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        train_folder, \n",
    "        target_size=(299, 299), batch_size=987)\n",
    "\n",
    "# get all the data in the directory split/validation (347 images), and reshape them\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        val_folder, \n",
    "        target_size=(299, 299), batch_size = 347)\n",
    "\n",
    "# get all the data in the directory split/test (624 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_folder, \n",
    "        target_size=(299, 299), batch_size = 624) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef5ef6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)\n",
    "val_images, val_labels = next(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93e1b09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 987\n",
      "Number of testing samples: 624\n",
      "Number of validation samples: 347\n",
      "train_images shape: (987, 299, 299, 3)\n",
      "train_labels shape: (987, 2)\n",
      "test_images shape: (624, 299, 299, 3)\n",
      "test_labels shape: (624, 2)\n",
      "val_images shape: (347, 299, 299, 3)\n",
      "val_labels shape: (347, 2)\n"
     ]
    }
   ],
   "source": [
    "# Explore dataset\n",
    "m_train = train_images.shape[0]\n",
    "num_px = train_images.shape[1]\n",
    "m_test = test_images.shape[0]\n",
    "m_val = val_images.shape[0]\n",
    "\n",
    "print (\"Number of training samples: \" + str(m_train))\n",
    "print (\"Number of testing samples: \" + str(m_test))\n",
    "print (\"Number of validation samples: \" + str(m_val))\n",
    "print (\"train_images shape: \" + str(train_images.shape))\n",
    "print (\"train_labels shape: \" + str(train_labels.shape))\n",
    "print (\"test_images shape: \" + str(test_images.shape))\n",
    "print (\"test_labels shape: \" + str(test_labels.shape))\n",
    "print (\"val_images shape: \" + str(val_images.shape))\n",
    "print (\"val_labels shape: \" + str(val_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e78b2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(987, 268203)\n",
      "(624, 268203)\n",
      "(347, 268203)\n"
     ]
    }
   ],
   "source": [
    "train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "test_img = test_images.reshape(test_images.shape[0], -1)\n",
    "val_img = val_images.reshape(val_images.shape[0], -1)\n",
    "\n",
    "print(train_img.shape)\n",
    "print(test_img.shape)\n",
    "print(val_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a48c1b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.reshape(train_labels[:,0], (train_img_number,1))\n",
    "test_y = np.reshape(test_labels[:,0], (test_img_number,1))\n",
    "val_y = np.reshape(val_labels[:,0], (val_img_number,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7647ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(100, activation='relu', input_shape=(train_img.shape[1],))) # 2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0ebb5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31/31 [==============================] - 17s 477ms/step - loss: 0.6922 - accuracy: 0.5238 - val_loss: 0.6895 - val_accuracy: 0.5677\n",
      "Epoch 2/10\n",
      "31/31 [==============================] - 12s 400ms/step - loss: 0.6922 - accuracy: 0.5238 - val_loss: 0.6893 - val_accuracy: 0.5677\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - 16s 498ms/step - loss: 0.6922 - accuracy: 0.5238 - val_loss: 0.6892 - val_accuracy: 0.5677\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - 10s 330ms/step - loss: 0.6921 - accuracy: 0.5238 - val_loss: 0.6891 - val_accuracy: 0.5677\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - 10s 316ms/step - loss: 0.6921 - accuracy: 0.5238 - val_loss: 0.6890 - val_accuracy: 0.5677\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - 10s 316ms/step - loss: 0.6921 - accuracy: 0.5238 - val_loss: 0.6889 - val_accuracy: 0.5677\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - 11s 333ms/step - loss: 0.6921 - accuracy: 0.5238 - val_loss: 0.6888 - val_accuracy: 0.5677\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - 11s 354ms/step - loss: 0.6921 - accuracy: 0.5238 - val_loss: 0.6887 - val_accuracy: 0.5677\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - 11s 349ms/step - loss: 0.6921 - accuracy: 0.5238 - val_loss: 0.6887 - val_accuracy: 0.5677\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - 11s 354ms/step - loss: 0.6921 - accuracy: 0.5238 - val_loss: 0.6886 - val_accuracy: 0.5677\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "histoire = model.fit(train_img,\n",
    "                    train_y,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val_img, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ebd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f1354b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN = models.Sequential()\n",
    "model_CNN.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(299 ,299,  3)))\n",
    "model_CNN.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model_CNN.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model_CNN.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model_CNN.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_CNN.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model_CNN.add(layers.Flatten())\n",
    "model_CNN.add(layers.Dense(64, activation='relu'))\n",
    "model_CNN.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_CNN.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a52d605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "31/31 [==============================] - 147s 5s/step - loss: 0.6673 - acc: 0.5907 - val_loss: 0.6756 - val_acc: 0.4496\n",
      "Epoch 2/30\n",
      "31/31 [==============================] - 138s 4s/step - loss: 0.6110 - acc: 0.6738 - val_loss: 0.5413 - val_acc: 0.8357\n",
      "Epoch 3/30\n",
      "12/31 [==========>...................] - ETA: 1:23 - loss: 0.5574 - acc: 0.7422"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bg/b5dfn0gj1lz88k7ms283yrlm0000gn/T/ipykernel_54873/1978626272.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(train_images,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     validation_data=(val_images, val_y))\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_CNN1 = model_CNN.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=30,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val_images, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3468e98",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "---\n",
    "For our finalized model we used LinearRegression because of the following reasons: \n",
    "1) It is easy to interprete.\n",
    "\n",
    "2) It have good recall compared to the other models, without much sacrifice in precision. \n",
    "\n",
    "\n",
    "Overall, this data tells us that injuries during car crashes can be predicted and we can see tha major factors that influence it. These modeling results correspond to our observations during data exploration phase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
